{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uPWdyLkkpbgg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lTQutar1pjNs"
   },
   "outputs": [],
   "source": [
    "labels = [\"0\",\"1\",\"2\",\"3\",\"4\"]\n",
    "img_size = 32\n",
    "def get_data(data_dir):\n",
    "    data = []\n",
    "    for label in labels:\n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iNqiQRtpk89",
    "outputId": "3fe12d6a-e0ca-457a-b08a-80d0c4572ac5"
   },
   "outputs": [],
   "source": [
    "train = get_data(r\"C:\\\\Users\\\\sadik\\\\OneDrive\\\\Desktop\\\\mushrat\\\\knee-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "Uqw12vpiqABT",
    "outputId": "c983f3fa-3fea-4fa9-ae4a-48638607aeb9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(train[1][0])\n",
    "plt.title(labels[train[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4461DWCoqCPI"
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "\n",
    "for feature, label in train:\n",
    "  x_train.append(feature)\n",
    "  y_train.append(label)\n",
    "x_train = np.array(x_train) / 255\n",
    "x_train.reshape(-1, img_size, img_size, 1)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yZdl1A3TqFQL"
   },
   "outputs": [],
   "source": [
    "# split with a stratified sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "(x_train, x_test, y_train, y_test) = train_test_split(x_train, y_train,\n",
    "    test_size=0.15, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkTBOgrdqNi7",
    "outputId": "6d615873-7295-4bed-9f60-48b6b74341c7"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iI1ToY4qPgO",
    "outputId": "58fced4f-87cc-4681-d2fe-a576c0f70952"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9X65w08360ze"
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(\n",
    "    y_train, num_classes=5, dtype='float32'\n",
    ")\n",
    "y_test = keras.utils.to_categorical(\n",
    "    y_test, num_classes=5, dtype='float32'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0JRCI5n0qRYz"
   },
   "outputs": [],
   "source": [
    "positional_emb = True\n",
    "conv_layers = 2\n",
    "projection_dim = 128\n",
    "\n",
    "num_heads = 2\n",
    "transformer_units = [\n",
    "    projection_dim,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 1\n",
    "stochastic_depth_rate = 0.1\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "image_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bTJIIAbjqTcc"
   },
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "input_shape = (32, 32, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFxMSXZebuFe",
    "outputId": "9a88d5b6-b13b-4319-b2ca-244a4398a82b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (93697, 32, 32, 3) - y_train shape: (93697, 5)\n",
      "x_test shape: (16535, 32, 32, 3) - y_test shape: (16535, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AcB000FOqamH"
   },
   "outputs": [],
   "source": [
    "class CCTTokenizer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=4,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pooling_kernel_size=4,\n",
    "        pooling_stride=2,\n",
    "        num_conv_layers=conv_layers,\n",
    "        num_output_channels=[64, 128],\n",
    "        positional_emb=positional_emb,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CCTTokenizer, self).__init__(**kwargs)\n",
    "\n",
    "        # This is our tokenizer.\n",
    "        self.conv_model = keras.Sequential()\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_model.add(\n",
    "                layers.Conv2D(\n",
    "                    num_output_channels[i],\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding=\"valid\",\n",
    "                    use_bias=False,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "            )\n",
    "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
    "            self.conv_model.add(\n",
    "                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
    "            )\n",
    "\n",
    "        self.positional_emb = positional_emb\n",
    "\n",
    "    def call(self, images):\n",
    "        outputs = self.conv_model(images)\n",
    "        # After passing the images through our mini-network the spatial dimensions\n",
    "        # are flattened to form sequences.\n",
    "        reshaped = tf.reshape(\n",
    "            outputs,\n",
    "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
    "        )\n",
    "        return reshaped\n",
    "\n",
    "    def positional_embedding(self, image_size):\n",
    "        # Positional embeddings are optional in CCT. Here, we calculate\n",
    "        # the number of sequences and initialize an `Embedding` layer to\n",
    "        # compute the positional embeddings later.\n",
    "        if self.positional_emb:\n",
    "            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
    "            dummy_outputs = self.call(dummy_inputs)\n",
    "            sequence_length = tf.shape(dummy_outputs)[1]\n",
    "            projection_dim = tf.shape(dummy_outputs)[-1]\n",
    "\n",
    "            embed_layer = layers.Embedding(\n",
    "                input_dim=sequence_length, output_dim=projection_dim\n",
    "            )\n",
    "            return embed_layer, sequence_length\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PFLyTlEEqhwv"
   },
   "outputs": [],
   "source": [
    "# Referred from: github.com:rwightman/pytorch-image-models.\n",
    "class StochasticDepth(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super(StochasticDepth, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0r8TOIiyqjyt"
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EFWfh2-0ql_u"
   },
   "outputs": [],
   "source": [
    "# Note the rescaling layer. These layers have pre-defined inference behavior.\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(scale=1.0 / 255),\n",
    "        layers.RandomCrop(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5I0BDxrkqpFG"
   },
   "outputs": [],
   "source": [
    "def create_cct_model(\n",
    "    image_size=image_size,\n",
    "    input_shape=input_shape,\n",
    "    num_heads=num_heads,\n",
    "    projection_dim=projection_dim,\n",
    "    transformer_units=transformer_units,\n",
    "):\n",
    "\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "\n",
    "    # Encode patches.\n",
    "    cct_tokenizer = CCTTokenizer()\n",
    "    encoded_patches = cct_tokenizer(augmented)\n",
    "\n",
    "    # Apply positional embedding.\n",
    "    if positional_emb:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    # Calculate Stochastic Depth probabilities.\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for i in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Apply sequence pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
    "    weighted_representation = tf.matmul(\n",
    "        attention_weights, representation, transpose_a=True\n",
    "    )\n",
    "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
    "\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(weighted_representation)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "vY_iJAlXqt0s",
    "outputId": "46620903-0003-4a1c-ac6a-8ef8f47907de",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name=\"Adam\"\n",
    ")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, label_smoothing=0.1\n",
    "        ),\n",
    "        metrics=[\n",
    "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = r\"C:\\\\Users\\\\sadik\\\\OneDrive\\\\Desktop\\\\mushrat\\\\accuracy\\\\cct\\\\cctmodel2.h5\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.10,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "    # as I've trained my model on MNIST as odd or even (binary classes)\n",
    "    target_names = [\"0\", \"1\", \"2\",\"3\",\"4\"]\n",
    "\n",
    "\n",
    "    # get predict prob and label\n",
    "    ypred = model.predict(x_test, verbose=1)\n",
    "    ypred = np.argmax(ypred, axis=1)\n",
    "    print(classification_report(np.argmax(y_test, axis=1), ypred, target_names=target_names))\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "\n",
    "    cm = confusion_matrix(np.argmax(y_test, axis=1), ypred)\n",
    "    cm = pd.DataFrame(cm, range(5),range(5))\n",
    "    plt.figure(figsize = (10,10))\n",
    "\n",
    "    sns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
    "    plt.show()\n",
    "    return history\n",
    "\n",
    "\n",
    "cct_model = create_cct_model()\n",
    "history = run_experiment(cct_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name=\"Adam\"\n",
    ")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, label_smoothing=0.1\n",
    "        ),\n",
    "        metrics=[\n",
    "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = r\"C:\\\\Users\\\\sadik\\\\OneDrive\\\\Desktop\\\\mushrat\\\\accuracy\\\\cct\\\\cctmodel2.h5\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.10,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "    # as I've trained my model on MNIST as odd or even (binary classes)\n",
    "    target_names = [\"0\", \"1\", \"2\",\"3\",\"4\"]\n",
    "\n",
    "\n",
    "    # get predict prob and label\n",
    "    ypred = model.predict(x_test, verbose=1)\n",
    "    ypred = np.argmax(ypred, axis=1)\n",
    "    print(classification_report(np.argmax(y_test, axis=1), ypred, target_names=target_names))\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "\n",
    "    cm = confusion_matrix(np.argmax(y_test, axis=1), ypred)\n",
    "    cm = pd.DataFrame(cm, range(5),range(5))\n",
    "    plt.figure(figsize = (10,10))\n",
    "\n",
    "    sns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
    "    plt.show()\n",
    "    return history,cm\n",
    "\n",
    "\n",
    "cct_model = create_cct_model()\n",
    "history, cm = run_experiment(cct_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Rsd1klXSP1HZ",
    "outputId": "7d6e2306-c57a-4290-c374-afc5fa62d0da"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Vk57M4ErS1kt",
    "outputId": "ee962d19-64dc-4cb8-ef7b-e21839a2ba6a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Train and Validation accuracy Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
